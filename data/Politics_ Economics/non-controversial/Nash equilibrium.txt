Nash equilibrium In game theory, the Nash equilibrium is a solution concept of
a non-cooperative game involving two or more players, in which each player is
assumed to know the equilibrium strategies of the other players, and no player
has anything to gain by changing only his own strategy unilaterally. If each
player has chosen a strategy and no player can benefit by changing his or her
strategy while the other players keep theirs unchanged, then the current set of
strategy choices and the corresponding payoffs constitute a Nash equilibrium.
Stated simply, Amy and Phil are in Nash equilibrium if Amy is making the best
decision she can, taking into account Phil's decision, and Phil is making the
best decision he can, taking into account Amy's decision. Likewise, a group of
players are in Nash equilibrium if each one is making the best decision that he
or she can, taking into account the decisions of the others. Applications. Game
theorists use the Nash equilibrium concept to analyze the outcome of the
strategic interaction of several decision makers. In other words, it provides a
way of predicting what will happen if several people or several institutions
are making decisions at the same time, and if the outcome depends on the
decisions of the others. The simple insight underlying John Nash's idea is that
we cannot predict the result of the choices of multiple decision makers if we
analyze those decisions in isolation. Instead, we must ask what each player
would do, "taking into account" the decision-making of the others. Nash
equilibrium has been used to analyze hostile situations like war and arms races
(see Prisoner's dilemma), and also how conflict may be mitigated by repeated
interaction (see Tit-for-tat). It has also been used to study to what extent
people with different preferences can cooperate (see Battle of the sexes), and
whether they will take risks to achieve a cooperative outcome (see Stag hunt).
It has been used to study the adoption of technical standards, and also the
occurrence of bank runs and currency crises (see Coordination game). Other
applications include traffic flow (see Wardrop's principle), how to organize
auctions (see auction theory), the outcome of efforts exerted by multiple
parties in the education process, regulatory legislation such as environmental
regulations (see Tragedy of the Commons), and even penalty kicks in soccer (see
Matching pennies). History. The Nash equilibrium was named after John Forbes
Nash. A version of the Nash equilibrium concept was first used by Antoine
Augustin Cournot in his theory of oligopoly (1838). In Cournot's theory, firms
choose how much output to produce to maximize their own profit. However, the
best output for one firm depends on the outputs of others. A Cournot
equilibrium occurs when each firm's output maximizes its profits given the
output of the other firms, which is a pure strategy Nash Equilibrium. The
modern game-theoretic concept of Nash Equilibrium is instead defined in terms
of mixed strategies, where players choose a probability distribution over
possible actions. The concept of the mixed strategy Nash Equilibrium was
introduced by John von Neumann and Oskar Morgenstern in their 1944 book "The
Theory of Games and Economic Behavior". However, their analysis was restricted
to the special case of zero-sum games. They showed that a mixed-strategy Nash
Equilibrium will exist for any zero-sum game with a finite set of actions. The
contribution of John Forbes Nash in his 1951 article "Non-Cooperative Games"
was to define a mixed strategy Nash Equilibrium for any game with a finite set
of actions and prove that at least one (mixed strategy) Nash Equilibrium must
exist in such a game. Since the development of the Nash equilibrium concept,
game theorists have discovered that it makes misleading predictions (or fails
to make a unique prediction) in certain circumstances. Therefore they have
proposed many related solution concepts (also called 'refinements' of Nash
equilibrium) designed to overcome perceived flaws in the Nash concept. One
particularly important issue is that some Nash equilibria may be based on
threats that are not 'credible'. Therefore, in 1965 Reinhard Selten proposed
subgame perfect equilibrium as a refinement that eliminates equilibria which
depend on non-credible threats. Other extensions of the Nash equilibrium
concept have addressed what happens if a game is repeated, or what happens if a
game is played in the absence of perfect information. However, subsequent
refinements and extensions of the Nash equilibrium concept share the main
insight on which Nash's concept rests: all equilibrium concepts analyze what
choices will be made when each player takes into account the decision-making of
others. Definitions. Informal definition. Informally, a set of strategies is a
Nash equilibrium if no player can do better by unilaterally changing his or her
strategy. To see what this means, imagine that each player is told the
strategies of the others. Suppose then that each player asks himself or
herself: "Knowing the strategies of the other players, and treating the
strategies of the other players as set in stone, can I benefit by changing my
strategy?" If any player would answer "Yes", then that set of strategies is not
a Nash equilibrium. But if every player prefers not to switch (or is
indifferent between switching and not) then the set of strategies is a Nash
equilibrium. Thus, each strategy in a Nash equilibrium is a best response to
all other strategies in that equilibrium. The Nash equilibrium may sometimes
appear non-rational in a third-person perspective. This is because it may
happen that a Nash equilibrium is not Pareto optimal. The Nash equilibrium may
also have non-rational consequences in sequential games because players may
"threaten" each other with non-rational moves. For such games the subgame
perfect Nash equilibrium may be more meaningful as a tool of analysis. Formal
definition. Let "(S, f)" be a game with "n" players, where "Si" is the strategy
set for player "i", "S=S1 Ã— S2 ... Ã— Sn" is the set of strategy profiles and
"f=(f1(x), ..., fn(x))" is the payoff function for "x" formula_1 "S". Let "xi"
be a strategy profile of player "i" and "x-i" be a strategy profile of all
players except for player "i". When each player "i" formula_1 {1, ..., n}
chooses strategy "xi" resulting in strategy profile "x = (x1, ..., xn)" then
player "i" obtains payoff "fi(x)". Note that the payoff depends on the strategy
profile chosen, i.e., on the strategy chosen by player "i" as well as the
strategies chosen by all the other players. A strategy profile "x*" formula_1
"S" is a Nash equilibrium (NE) if no unilateral deviation in strategy by any
single player is profitable for that player, that is A game can have either a
pure-strategy or a mixed Nash Equilibrium, (in the latter a pure strategy is
chosen stochastically with a fixed frequency). Nash proved that if we allow
mixed strategies, then every game with a finite number of players in which each
player can choose from finitely many pure strategies has at least one Nash
equilibrium. When the inequality above holds strictly (with formula_5 instead
of formula_6) for all players and all feasible alternative strategies, then the
equilibrium is classified as a strict Nash equilibrium. If instead, for some
player, there is exact equality between formula_7 and some other strategy in
the set formula_8, then the equilibrium is classified as a weak Nash
equilibrium. Examples. Coordination game. The "coordination game" is a classic
(symmetric) two player, two strategy game, with an example payoff matrix shown
to the right. The players should thus coordinate, both adopting strategy A, to
receive the highest payoff; i.e., 4. If both players chose strategy B though,
there is still a Nash equilibrium. Although each player is awarded less than
optimal payoff, neither player has incentive to change strategy due to a
reduction in the immediate payoff (from 2 to 1). A famous example of this type
of game was called the Stag hunt; in the game two players may choose to hunt a
stag or a rabbit, the former providing more meat (4 utility units) than the
latter (1 utility unit). The caveat is that the stag must be cooperatively
hunted, so if one player attempts to hunt the stag, while the other hunts the
rabbit, he will fail in hunting (0 utility units), whereas if they both hunt it
they will split the payload (2, 2). The game hence exhibits two equilibria at
(stag, stag) and (rabbit, rabbit) and hence the players' optimal strategy
depend on their expectation on what the other player may do. If one hunter
trusts that the other will hunt the stag, he should hunt the stag; however if
he suspects that the other will hunt the rabbit, he should hunt the rabbit.
This game was used as an analogy for social cooperation, since much of the
benefit that people gain in society depends upon people cooperating and
implicitly trusting one another to act in a manner corresponding with
cooperation. Another example of a coordination game is the setting where two
technologies are available to two firms with compatible products, and they have
to elect a strategy to become the market standard. If both firms agree on the
chosen technology, high sales are expected for both firms. If the firms do not
agree on the standard technology, few sales result. Both strategies are Nash
equilibria of the game. In this case there are two pure strategy Nash
equilibria, when both choose to either drive on the left or on the right. If we
admit mixed strategies (where a pure strategy is chosen at random, subject to
some fixed probability), then there are three Nash equilibria for the same
case: two we have seen from the pure-strategy form, where the probabilities are
(0%,100%) for player one, (0%, 100%) for player two; and (100%, 0%) for player
one, (100%, 0%) for player two respectively. We add another where the
probabilities for each player is (50%, 50%). Prisoner's dilemma. The Prisoner's
Dilemma has a similar matrix as depicted for the Coordination Game, but the
maximum reward for each player (in this case, 5) is only obtained when the
players' decisions are different. Each player improves his own situation by
switching from "Cooperating" to "Defecting," given knowledge that the other
player's best decision is to "Defect." The Prisoner's Dilemma thus has a single
Nash Equilibrium: both players choosing to defect. What has long made this an
interesting case to study is the fact that this scenario is globally inferior
to "Both Cooperating." That is, both players would be better off if they both
chose to "Cooperate" instead of both choosing to defect. However, each player
could improve his own situation by breaking the mutual cooperation, no matter
how the other player possibly (or certainly) changes his decision. Network
traffic. An application of Nash equilibria is in determining the expected flow
of traffic in a network. Consider the graph on the right. If we assume that
there are formula_9 "cars" traveling from A to D, what is the expected
distribution of traffic in the network? This situation can be modeled as a
"game" where every traveler has a choice of 3 strategies, where each strategy
is a route from A to D (either formula_10, formula_11, or formula_12). The
"payoff" of each strategy is the travel time of each route. In the graph on the
right, a car travelling via formula_10 experiences travel time of formula_14,
where formula_9 is the number of cars traveling on edge formula_16. Thus,
payoffs for any given strategy depend on the choices of the other players, as
is usual. However, the goal in this case is to minimize travel time, not
maximize it. Equilibrium will occur when the time on all paths is exactly the
same. When that happens, no single driver has any incentive to switch routes,
since it can only add to his/her travel time. For the graph on the right, if,
for example, 100 cars are travelling from A to D, then equilibrium will occur
when 25 drivers travel via formula_10, 50 via formula_11, and 25 via
formula_12. Every driver now has a total travel time of 3.75. Notice that this
distribution is not, actually, socially optimal. If the 100 cars agreed that 50
travel via formula_10 and the other 50 through formula_12, then travel time for
any single car would actually be 3.5 which is less than 3.75. This is also the
Nash equilibrium if the path between B and C is removed, which means that
adding an additional possible route can decrease the efficiency of the system,
a phenomenon known as Braess's Paradox. Competition game. This can be
illustrated by a two-player game in which both players simultaneously choose an
integer from 0 to 3 and they both win the smaller of the two numbers in points.
In addition, if one player chooses a larger number than the other, then he/she
has to give up two points to the other. This game has a unique pure-strategy
Nash equilibrium: both players choosing 0 (highlighted in light red). Any other
choice of strategies can be improved if one of the players lowers his number to
one less than the other player's number. In the table to the right, for
example, when starting at the green square it is in player 1's interest to move
to the purple square by choosing a smaller number, and it is in player 2's
interest to move to the blue square by choosing a smaller number. If the game
is modified so that the two players win the named amount if they both choose
the same number, and otherwise win nothing, then there are 4 Nash equilibria
(0,0...1,1...2,2...and 3,3). Nash equilibria in a payoff matrix. There is an
easy numerical way to identify Nash equilibria on a payoff matrix. It is
especially helpful in two-person games where players have more than two
strategies. In this case formal analysis may become too long. This rule does
not apply to the case where mixed (stochastic) strategies are of interest. The
rule goes as follows: if the first payoff number, in the duplet of the cell, is
the maximum of the column of the cell and if the second number is the maximum
of the row of the cell - then the cell represents a Nash equilibrium. Using the
rule, we can very quickly (much faster than with formal analysis) see that the
Nash Equilibria cells are (B,A), (A,B), and (C,C). Indeed, for cell (B,A) 40 is
the maximum of the first column and 25 is the maximum of the second row. For
(A,B) 25 is the maximum of the second column and 40 is the maximum of the first
row. Same for cell (C,C). For other cells, either one or both of the duplet
members are not the maximum of the corresponding rows and columns. This said,
the actual mechanics of finding equilibrium cells is obvious: find the maximum
of a column and check if the second member of the pair is the maximum of the
row. If these conditions are met, the cell represents a Nash Equilibrium. Check
all columns this way to find all NE cells. An NÃ—N matrix may have between 0
and NÃ—N pure strategy Nash equilibria. Stability. The concept of stability,
useful in the analysis of many kinds of equilibria, can also be applied to Nash
equilibria. If these cases are both met, then a player with the small change in
his mixed-strategy will return immediately to the Nash equilibrium. The
equilibrium is said to be stable. If condition one does not hold then the
equilibrium is unstable. If only condition one holds then there are likely to
be an infinite number of optimal strategies for the player who changed. John
Nash showed that the latter situation could not arise in a range of well-
defined games. In the "driving game" example above there are both stable and
unstable equilibria. The equilibria involving mixed-strategies with 100%
probabilities are stable. If either player changes his probabilities slightly,
they will be both at a disadvantage, and his opponent will have no reason to
change his strategy in turn. The (50%,50%) equilibrium is unstable. If either
player changes his probabilities, then the other player immediately has a
better strategy at either (0%, 100%) or (100%, 0%). Stability is crucial in
practical applications of Nash equilibria, since the mixed-strategy of each
player is not perfectly known, but has to be inferred from statistical
distribution of his actions in the game. In this case unstable equilibria are
very unlikely to arise in practice, since any minute change in the proportions
of each strategy seen will lead to a change in strategy and the breakdown of
the equilibrium. The Nash equilibrium defines stability only in terms of
unilateral deviations. In cooperative games such a concept is not convincing
enough. Strong Nash equilibrium allows for deviations by every conceivable
coalition. Formally, a Strong Nash equilibrium is a Nash equilibrium in which
no coalition, taking the actions of its complements as given, can cooperatively
deviate in a way that benefits all of its members. However, the Strong Nash
concept is sometimes perceived as too "strong" in that the environment allows
for unlimited private communication. In fact, Strong Nash equilibrium has to be
Pareto efficient. As a result of these requirements, Strong Nash is too rare to
be useful in many branches of game theory. However, in games such as elections
with many more players than possible outcomes, it can be more common than a
stable equilibrium. A refined Nash equilibrium known as coalition-proof Nash
equilibrium (CPNE) occurs when players cannot do better even if they are
allowed to communicate and make "self-enforcing" agreement to deviate. Every
correlated strategy supported by iterated strict dominance and on the Pareto
frontier is a CPNE. Further, it is possible for a game to have a Nash
equilibrium that is resilient against coalitions less than a specified size, k.
CPNE is related to the theory of the core. Finally in the eighties, building
with great depth on such ideas Mertens-stable equilibria were introduced as a
solution concept. Mertens stable equilibria satisfy both forward induction and
backward induction. In a Game theory context stable equilibria now usually
refer to Mertens stable equilibria. Occurrence. Where the conditions are met.
Due to the limited conditions in which NE can actually be observed, they are
rarely treated as a guide to day-to-day behaviour, or observed in practice in
human negotiations. However, as a theoretical concept in economics and
evolutionary biology, the NE has explanatory power. The payoff in economics is
utility (or sometimes money), and in evolutionary biology gene transmission,
both are the fundamental bottom line of survival. Researchers who apply games
theory in these fields claim that strategies failing to maximize these for
whatever reason will be competed out of the market or environment, which are
ascribed the ability to test all strategies. This conclusion is drawn from the
"stability" theory above. In these situations the assumption that the strategy
observed is actually a NE has often been borne out by research. NE and non-
credible threats. The Nash equilibrium is a superset of the subgame perfect
Nash equilibrium. The subgame perfect equilibrium in addition to the Nash
Equilibrium requires that the strategy also is a Nash equilibrium in every
subgame of that game. This eliminates all non-credible threats, that is,
strategies that contain non-rational moves in order to make the counter-player
change his strategy. The image to the right shows a simple sequential game that
illustrates the issue with subgame imperfect Nash equilibria. In this game
player one chooses left(L) or right(R), which is followed by player two being
called upon to be kind (K) or unkind (U) to player one, However, player two
only stands to gain from being unkind if player one goes left. If player one
goes right the rational player two would de facto be kind to him in that
subgame. However, The non-credible threat of being unkind at 2(2) is still part
of the blue (L, (U,U)) Nash equilibrium. Therefore, if rational behavior can be
expected by both parties the subgame perfect Nash equilibrium may be a more
meaningful solution concept when such dynamic inconsistencies arise. Proof of
existence. Proof using the Kakutani fixed point theorem. Nash's original proof
(in his thesis) used Brouwer's fixed point theorem (e.g., see below for a
variant). We give a simpler proof via the Kakutani fixed point theorem,
following Nash's 1950 paper (he credits David Gale with the observation that
such a simplification is possible). To prove the existence of a Nash
Equilibrium, let formula_22 be the best response of player i to the strategies
of all other players. Here, formula_24, where formula_25, is a mixed strategy
profile in the set of all mixed strategies and formula_26 is the payoff
function for player i. Define a set-valued function formula_27 such that
formula_28. The existence of a Nash Equilibrium is equivalent to formula_29
having a fixed point. Kakutani's fixed point theorem guarantees the existence
of a fixed point if the following four conditions are satisfied. Condition 1.
is satisfied from the fact that formula_34 is a simplex and thus compact.
Convexity follows from players' ability to mix strategies. formula_35 is
nonempty as long as players have strategies. Condition 2. is satisfied because
players maximize expected payoffs which is continuous function over a compact
set. The Weierstrass Extreme Value Theorem guarantees that there is always a
maximum value. Condition 3. is satisfied as a result of mixed strategies.
Suppose formula_36, then formula_37. i.e. if two strategies maximize payoffs,
then a mix between the two strategies will yield the same payoff. Condition 4.
is satisfied by way of Berge's maximum theorem. Because formula_26 is
continuous and compact, formula_39 is upper hemicontinuous. Therefore, there
exists a fixed point in formula_29 and a Nash Equilibrium. When Nash made this
point to John von Neumann in 1949, von Neumann famously dismissed it with the
words, "That's trivial, you know. That's just a fixed point theorem." (See
Nasar, 1998, p.Â 94.) Alternate proof using the Brouwer fixed-point theorem. We
have a game formula_41 where formula_42 is the number of players and formula_43
is the action set for the players. All of the action sets formula_44 are
finite. Let formula_45 denote the set of mixed strategies for the players. The
finiteness of the formula_44s ensures the compactness of formula_47. We can now
define the gain functions. For a mixed strategy formula_48, we let the gain for
player formula_49 on action formula_50 be The gain function represents the
benefit a player gets by unilaterally changing his strategy. We now define
formula_52 where for formula_54. We see that We now use formula_56 to define
formula_57 as follows. Let for formula_50. It is easy to see that each
formula_60 is a valid mixed strategy in formula_61. It is also easy to check
that each formula_60 is a continuous function of formula_63, and hence
formula_64 is a continuous function. Now formula_47 is the cross product of a
finite number of compact convex sets, and so we get that formula_47 is also
compact and convex. Therefore we may apply the Brouwer fixed point theorem to
formula_64. So formula_64 has a fixed point in formula_47, call it formula_70.
I claim that formula_70 is a Nash Equilibrium in formula_72. For this purpose,
it suffices to show that This simply states the each player gains no benefit by
unilaterally changing his strategy which is exactly the necessary condition for
being a Nash Equilibrium. Now assume that the gains are not all zero.
Therefore, formula_74, formula_75, and formula_50 such that formula_77. Note
then that So let formula_79. Also we shall denote formula_80 as the gain vector
indexed by actions in formula_44. Since formula_82 we clearly have that
formula_83. Therefore we see that Since formula_86 we have that formula_87 is
some positive scaling of the vector formula_88. Now I claim that formula_90. To
see this, we first note that if formula_77 then this is true by definition of
the gain function. Now assume that formula_92. By our previous statements we
have that and so the left term is zero, giving us that the entire expression is
formula_94 as needed. So we finally have that where the last inequality follows
since formula_87 is a non-zero vector. But this is a clear contradiction, so
all the gains must indeed be zero. Therefore formula_70 is a Nash Equilibrium
for formula_72 as needed. Computing Nash equilibria. If a player A has a
dominant strategy formula_103 then there exists a Nash equilibrium in which A
plays formula_103. In the case of two players A and B, there exists a Nash
equilibrium in which A plays formula_103 and B plays a best response to
formula_103. If formula_103 is a strictly dominant strategy, A plays
formula_103 in all Nash equilibria. If both A and B have strictly dominant
strategies, there exists a unique Nash equilibrium in which each plays his
strictly dominant strategy. In games with mixed strategy Nash equilibria, the
probability of a player choosing any particular strategy can be computed by
assigning a variable to each strategy that represents a fixed probability for
choosing that strategy. In order for a player to be willing to randomize, his
expected payoff for each strategy should be the same. In addition, the sum of
the probabilities for each strategy of a particular player should be 1. This
creates a system of equations from which the probabilities of choosing each
strategy can be derived. Examples. In the matching pennies game, player A loses
a point to B if A and B play the same strategy and wins a point from B if they
play different strategies. To compute the mixed strategy Nash equilibrium,
assign A the probability p of playing H and (1âˆ’p) of playing T, and assign B
the probability q of playing H and (1âˆ’q) of playing T. Thus a mixed strategy
Nash equilibrium in this game is for each player to randomly choose H or T with
equal probability.
